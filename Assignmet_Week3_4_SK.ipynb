{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jewish-allergy",
   "metadata": {},
   "source": [
    "# Assignment Week 3-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-announcement",
   "metadata": {},
   "source": [
    "Two similar applications employing a scalable 3D ResNet architecture learn to predict the subject’s age (regression) or the subject’s sex (classification) from T1–weighted brain MR images from the IXI database. The main difference between this applications is the loss function: While we train the regression network to predict the age as a continuous variable with a L2-loss (the mean squared differences between the predicted age and the real age), we use a categorical cross-entropy loss to predict the class of the sex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-catch",
   "metadata": {},
   "source": [
    "Downloading data for the example IXI_HH_sex_classification_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adult-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Download and extract the IXI Hammersmith Hospital 3T dataset\n",
    "\n",
    "url: http://brain-development.org/ixi-dataset/\n",
    "ref: IXI – Information eXtraction from Images (EPSRC GR/S21533/02)\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future.standard_library import install_aliases  # py 2/3 compatability\n",
    "install_aliases()\n",
    "\n",
    "from urllib.request import FancyURLopener\n",
    "\n",
    "import os.path\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "DOWNLOAD_IMAGES = True\n",
    "EXTRACT_IMAGES = True\n",
    "PROCESS_OTHER = True\n",
    "RESAMPLE_IMAGES = True\n",
    "CLEAN_UP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_image(itk_image, out_spacing=(1.0, 1.0, 1.0), is_label=False):\n",
    "    original_spacing = itk_image.GetSpacing()\n",
    "    original_size = itk_image.GetSize()\n",
    "\n",
    "    out_size = [int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n",
    "                int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n",
    "                int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2])))]\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size)\n",
    "    resample.SetOutputDirection(itk_image.GetDirection())\n",
    "    resample.SetOutputOrigin(itk_image.GetOrigin())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(itk_image)\n",
    "\n",
    "\n",
    "def reslice_image(itk_image, itk_ref, is_label=False):\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetReferenceImage(itk_ref)\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(itk_image)\n",
    "\n",
    "\n",
    "urls = {}\n",
    "urls['t1'] = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar'\n",
    "urls['t2'] = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar'\n",
    "urls['pd'] = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-PD.tar'\n",
    "urls['mra'] = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-MRA.tar'\n",
    "urls['demographic'] = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI.xls'\n",
    "\n",
    "fnames = {}\n",
    "fnames['t1'] = 't1.tar'\n",
    "fnames['t2'] = 't2.tar'\n",
    "fnames['pd'] = 'pd.tar'\n",
    "fnames['mra'] = 'mra.tar'\n",
    "fnames['demographic'] = 'demographic.xls'\n",
    "\n",
    "\n",
    "if DOWNLOAD_IMAGES:\n",
    "    # Download all IXI data\n",
    "    for key, url in urls.items():\n",
    "\n",
    "        if not os.path.isfile(fnames[key]):\n",
    "            print('Downloading {} from {}'.format(fnames[key], url))\n",
    "            curr_file = FancyURLopener()\n",
    "            curr_file.retrieve(url, fnames[key])\n",
    "        else:\n",
    "            print('File {} already exists. Skipping download.'.format(\n",
    "                fnames[key]))\n",
    "\n",
    "if EXTRACT_IMAGES:\n",
    "    # Extract the HH subset of IXI\n",
    "    for key, fname in fnames.items():\n",
    "\n",
    "        if (fname.endswith('.tar')):\n",
    "            print('Extracting IXI HH data from {}.'.format(fnames[key]))\n",
    "            output_dir = os.path.join('./orig/', key)\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            t = tarfile.open(fname, 'r')\n",
    "            for member in t.getmembers():\n",
    "                if '-HH-' in member.name:\n",
    "                    t.extract(member, output_dir)\n",
    "\n",
    "\n",
    "if PROCESS_OTHER:\n",
    "    # Process the demographic xls data and save to csv\n",
    "    xls = pd.ExcelFile('demographic.xls')\n",
    "    print(xls.sheet_names)\n",
    "\n",
    "    df = xls.parse('Table')\n",
    "    for index, row in df.iterrows():\n",
    "        IXI_id = 'IXI{:03d}'.format(row['IXI_ID'])\n",
    "        df.loc[index, 'IXI_ID'] = IXI_id\n",
    "\n",
    "        t1_exists = len(glob.glob('./orig/t1/{}*.nii.gz'.format(IXI_id)))\n",
    "        t2_exists = len(glob.glob('./orig/t2/{}*.nii.gz'.format(IXI_id)))\n",
    "        pd_exists = len(glob.glob('./orig/pd/{}*.nii.gz'.format(IXI_id)))\n",
    "        mra_exists = len(glob.glob('./orig/mra/{}*.nii.gz'.format(IXI_id)))\n",
    "\n",
    "        # Check if each entry is complete and drop if not\n",
    "        # if not t1_exists and not t2_exists and not pd_exists and not mra\n",
    "        # exists:\n",
    "        if not (t1_exists and t2_exists and pd_exists and mra_exists):\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    # Write to csv file\n",
    "    df.to_csv('demographic_HH.csv', index=False)\n",
    "\n",
    "if RESAMPLE_IMAGES:\n",
    "    # Resample the IXI HH T2 images to 1mm isotropic and reslice all\n",
    "    # others to it\n",
    "    df = pd.read_csv('demographic_HH.csv', dtype=object, keep_default_na=False,\n",
    "                     na_values=[]).values\n",
    "\n",
    "    for i in df:\n",
    "        IXI_id = i[0]\n",
    "        print('Resampling {}'.format(IXI_id))\n",
    "\n",
    "        t1_fn = glob.glob('./orig/t1/{}*.nii.gz'.format(IXI_id))[0]\n",
    "        t2_fn = glob.glob('./orig/t2/{}*.nii.gz'.format(IXI_id))[0]\n",
    "        pd_fn = glob.glob('./orig/pd/{}*.nii.gz'.format(IXI_id))[0]\n",
    "        mra_fn = glob.glob('./orig/mra/{}*.nii.gz'.format(IXI_id))[0]\n",
    "\n",
    "        t1 = sitk.ReadImage(t1_fn)\n",
    "        t2 = sitk.ReadImage(t2_fn)\n",
    "        pd = sitk.ReadImage(pd_fn)\n",
    "        mra = sitk.ReadImage(mra_fn)\n",
    "\n",
    "        # Resample to 1mm isotropic resolution\n",
    "        t2_1mm = resample_image(t2)\n",
    "        t1_1mm = reslice_image(t1, t2_1mm)\n",
    "        pd_1mm = reslice_image(pd, t2_1mm)\n",
    "        mra_1mm = reslice_image(mra, t2_1mm)\n",
    "\n",
    "        output_dir = os.path.join('./1mm/', IXI_id)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        print('T1: {} {}'.format(t1_1mm.GetSize(), t1_1mm.GetSpacing()))\n",
    "        print('T2: {} {}'.format(t2_1mm.GetSize(), t2_1mm.GetSpacing()))\n",
    "        print('PD: {} {}'.format(pd_1mm.GetSize(), pd_1mm.GetSpacing()))\n",
    "        print('MRA: {} {}'.format(mra_1mm.GetSize(), mra_1mm.GetSpacing()))\n",
    "\n",
    "        sitk.WriteImage(t1_1mm, os.path.join(output_dir, 'T1_1mm.nii.gz'))\n",
    "        sitk.WriteImage(t2_1mm, os.path.join(output_dir, 'T2_1mm.nii.gz'))\n",
    "        sitk.WriteImage(pd_1mm, os.path.join(output_dir, 'PD_1mm.nii.gz'))\n",
    "        sitk.WriteImage(mra_1mm, os.path.join(output_dir, 'MRA_1mm.nii.gz'))\n",
    "\n",
    "        # Resample to 2mm isotropic resolution\n",
    "        t2_2mm = resample_image(t2, out_spacing=[2.0, 2.0, 2.0])\n",
    "        t1_2mm = reslice_image(t1, t2_2mm)\n",
    "        pd_2mm = reslice_image(pd, t2_2mm)\n",
    "        mra_2mm = reslice_image(mra, t2_2mm)\n",
    "\n",
    "        output_dir = os.path.join('./2mm/', IXI_id)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        print('T1: {} {}'.format(t2_2mm.GetSize(), t1_2mm.GetSpacing()))\n",
    "        print('T2: {} {}'.format(t2_2mm.GetSize(), t2_2mm.GetSpacing()))\n",
    "        print('PD: {} {}'.format(pd_2mm.GetSize(), pd_2mm.GetSpacing()))\n",
    "        print('MRA: {} {}'.format(mra_2mm.GetSize(), mra_2mm.GetSpacing()))\n",
    "\n",
    "        sitk.WriteImage(t1_2mm, os.path.join(output_dir, 'T1_2mm.nii.gz'))\n",
    "        sitk.WriteImage(t2_2mm, os.path.join(output_dir, 'T2_2mm.nii.gz'))\n",
    "        sitk.WriteImage(pd_2mm, os.path.join(output_dir, 'PD_2mm.nii.gz'))\n",
    "        sitk.WriteImage(mra_2mm, os.path.join(output_dir, 'MRA_2mm.nii.gz'))\n",
    "\n",
    "\n",
    "if CLEAN_UP:\n",
    "    # Remove the .tar files\n",
    "    for key, fname in fnames.items():\n",
    "        if (fname.endswith('.tar')):\n",
    "            os.remove(fname)\n",
    "\n",
    "    # Remove all data in original resolution\n",
    "    os.system('rm -rf orig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-adventure",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "precious-driving",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'parameter_server_strategy_v2' from 'tensorflow.python.distribute' (C:\\Users\\Susheel\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f55581a89797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression_classification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresnet_3d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract_reader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dltk\\networks\\regression_classification\\resnet.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidual_unit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvanilla_residual_unit_3d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dltk\\core\\residual_unit.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                              \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                              \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                              \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                              \u001b[0muse_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                              \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdnn_logit_fn_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeansClustering\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearSDCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_fn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\run_config.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_training\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdistribute_coordinator_training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparameter_server_strategy_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'parameter_server_strategy_v2' from 'tensorflow.python.distribute' (C:\\Users\\Susheel\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import dltk\n",
    "\n",
    "from dltk.networks.regression_classification.resnet import resnet_3d\n",
    "from dltk.io.abstract_reader import Reader\n",
    "\n",
    "\n",
    "EVAL_EVERY_N_STEPS = 100\n",
    "EVAL_STEPS = 5\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE_CACHE_SIZE = 32\n",
    "\n",
    "MAX_STEPS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function to construct a tf.estimator.EstimatorSpec. It creates a\n",
    "        network given input features (e.g. from a dltk.io.abstract_reader) and\n",
    "        training targets (labels). Further, loss, optimiser, evaluation ops and\n",
    "        custom tensorboard summary ops can be added. For additional information,\n",
    "         please refer to https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#model_fn.\n",
    "\n",
    "    Args:\n",
    "        features (tf.Tensor): Tensor of input features to train from. Required\n",
    "            rank and dimensions are determined by the subsequent ops\n",
    "            (i.e. the network).\n",
    "        labels (tf.Tensor): Tensor of training targets or labels. Required rank\n",
    "            and dimensions are determined by the network output.\n",
    "        mode (str): One of the tf.estimator.ModeKeys: TRAIN, EVAL or PREDICT\n",
    "        params (dict, optional): A dictionary to parameterise the model_fn\n",
    "            (e.g. learning_rate)\n",
    "\n",
    "    Returns:\n",
    "        tf.estimator.EstimatorSpec: A custom EstimatorSpec for this experiment\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. create a model and its outputs\n",
    "    net_output_ops = resnet_3d(\n",
    "        features['x'],\n",
    "        num_res_units=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        filters=(16, 32, 64, 128, 256),\n",
    "        strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n",
    "        mode=mode,\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
    "\n",
    "    # 1.1 Generate predictions only (for `ModeKeys.PREDICT`)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=net_output_ops,\n",
    "            export_outputs={'out': tf.estimator.export.PredictOutput(net_output_ops)})\n",
    "\n",
    "    # 2. set up a loss function\n",
    "    one_hot_labels = tf.reshape(tf.one_hot(labels['y'], depth=NUM_CLASSES), [-1, NUM_CLASSES])\n",
    "\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=one_hot_labels,\n",
    "        logits=net_output_ops['logits'])\n",
    "\n",
    "    # 3. define a training op and ops for updating moving averages (i.e. for\n",
    "    # batch normalisation)\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimiser = tf.train.AdamOptimizer(\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        epsilon=1e-5)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimiser.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 4.1 (optional) create custom image summaries for tensorboard\n",
    "    my_image_summaries = {}\n",
    "    my_image_summaries['feat_t1'] = features['x'][0, 32, :, :, 0]\n",
    "\n",
    "    expected_output_size = [1, 96, 96, 1]  # [B, W, H, C]\n",
    "    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n",
    "     for name, image in my_image_summaries.items()]\n",
    "\n",
    "    # 4.2 (optional) track the rmse (scaled back by 100, see reader.py)\n",
    "    acc = tf.metrics.accuracy\n",
    "    prec = tf.metrics.precision\n",
    "    eval_metric_ops = {\"accuracy\": acc(labels['y'], net_output_ops['y_']),\n",
    "                       \"precision\": prec(labels['y'], net_output_ops['y_'])}\n",
    "\n",
    "    # 5. Return EstimatorSpec object\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=net_output_ops,\n",
    "                                      loss=loss,\n",
    "                                      train_op=train_op,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    np.random.seed(42)\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    print('Setting up...')\n",
    "\n",
    "    # Parse csv files for file names\n",
    "    all_filenames = pd.read_csv(\n",
    "        args.data_csv,\n",
    "        dtype=object,\n",
    "        keep_default_na=False,\n",
    "        na_values=[]).as_matrix()\n",
    "\n",
    "    train_filenames = all_filenames[:150]\n",
    "    val_filenames = all_filenames[150:]\n",
    "\n",
    "    # Set up a data reader to handle the file i/o.\n",
    "    reader_params = {'n_examples': 2,\n",
    "                     'example_size': [64, 96, 96],\n",
    "                     'extract_examples': True}\n",
    "\n",
    "    reader_example_shapes = {'features': {'x': reader_params['example_size'] + [NUM_CHANNELS]},\n",
    "                             'labels': {'y': [1]}}\n",
    "    reader = Reader(read_fn,\n",
    "                    {'features': {'x': tf.float32},\n",
    "                     'labels': {'y': tf.int32}})\n",
    "\n",
    "    # Get input functions and queue initialisation hooks for training and\n",
    "    # validation data\n",
    "    train_input_fn, train_qinit_hook = reader.get_inputs(\n",
    "        file_references=train_filenames,\n",
    "        mode=tf.estimator.ModeKeys.TRAIN,\n",
    "        example_shapes=reader_example_shapes,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n",
    "        params=reader_params)\n",
    "\n",
    "    val_input_fn, val_qinit_hook = reader.get_inputs(\n",
    "        file_references=val_filenames,\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        example_shapes=reader_example_shapes,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n",
    "        params=reader_params)\n",
    "\n",
    "    # Instantiate the neural network estimator\n",
    "    nn = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        model_dir=args.model_path,\n",
    "        params={\"learning_rate\": 0.001},\n",
    "        config=tf.estimator.RunConfig())\n",
    "\n",
    "    # Hooks for validation summaries\n",
    "    val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n",
    "        os.path.join(args.model_path, 'eval'))\n",
    "    step_cnt_hook = tf.train.StepCounterHook(every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "                                             output_dir=args.model_path)\n",
    "\n",
    "    print('Starting training...')\n",
    "    try:\n",
    "        for _ in range(MAX_STEPS // EVAL_EVERY_N_STEPS):\n",
    "            nn.train(\n",
    "                input_fn=train_input_fn,\n",
    "                hooks=[train_qinit_hook, step_cnt_hook],\n",
    "                steps=EVAL_EVERY_N_STEPS)\n",
    "\n",
    "            if args.run_validation:\n",
    "                results_val = nn.evaluate(\n",
    "                    input_fn=val_input_fn,\n",
    "                    hooks=[val_qinit_hook, val_summary_hook],\n",
    "                    steps=EVAL_STEPS)\n",
    "                print('Step = {}; val loss = {:.5f};'.format(\n",
    "                    results_val['global_step'],\n",
    "                    results_val['loss']))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    # When exporting we set the expected input shape to be arbitrary.\n",
    "    export_dir = nn.export_savedmodel(\n",
    "        export_dir_base=args.model_path,\n",
    "        serving_input_receiver_fn=reader.serving_input_receiver_fn(\n",
    "            {'features': {'x': [None, None, None, NUM_CHANNELS]},\n",
    "             'labels': {'y': [1]}}))\n",
    "    print('Model saved to {}.'.format(export_dir))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up argument parser\n",
    "    parser = argparse.ArgumentParser(description='Example: IXI HH resnet sex classification training')\n",
    "    parser.add_argument('--run_validation', default=True)\n",
    "    parser.add_argument('--restart', default=False, action='store_true')\n",
    "    parser.add_argument('--verbose', default=False, action='store_true')\n",
    "    parser.add_argument('--cuda_devices', '-c', default='0')\n",
    "\n",
    "    parser.add_argument('--model_path', '-p', default='/tmp/IXI_sex_classification/')\n",
    "    parser.add_argument('--data_csv', default='../../../data/IXI_HH/demographic_HH.csv')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set verbosity\n",
    "    if args.verbose:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    else:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    # GPU allocation options\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_devices\n",
    "\n",
    "    # Handle restarting and resuming training\n",
    "    if args.restart:\n",
    "        print('Restarting training from scratch.')\n",
    "        os.system('rm -rf {}'.format(args.model_path))\n",
    "\n",
    "    if not os.path.isdir(args.model_path):\n",
    "        os.system('mkdir -p {}'.format(args.model_path))\n",
    "    else:\n",
    "        print('Resuming training on model_path {}'.format(args.model_path))\n",
    "\n",
    "    # Call training\n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
